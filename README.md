# fastapicloud_dividend

A project created with FastAPI CLI.

## Quick Start

### Start the development server

```bash
uv run fastapi dev
```

Visit http://localhost:8000

### Deploy to FastAPI Cloud

> FastAPI Cloud is currently in private beta. Join the waitlist at https://fastapicloud.com

```bash
uv sync
uv run fastapi login
uv run fastapi deploy
```

## Project Structure

- `main.py` - Your FastAPI application
- `pyproject.toml` - Project dependencies

## Learn More

- [FastAPI Documentation](https://fastapi.tiangolo.com)
- [FastAPI Cloud](https://fastapicloud.com)
åœ¨çœŸå®å·¥ç¨‹é¡¹ç›®ä¸­ï¼Œä¸ä½¿ç”¨ LangChainã€LangGraphã€CrewAI ç­‰æ¡†æ¶ï¼Œçº¯ç”¨ Python + LLM API æ‰‹åŠ¨å®ç° AI Agent ä¸ä»…å®Œå…¨å¯è¡Œï¼Œè€Œä¸”åœ¨è®¸å¤šåœºæ™¯ä¸‹æ˜¯æ›´ä¼˜é€‰æ‹©ã€‚ Anthropic å®˜æ–¹æ˜ç¡®å»ºè®®å¼€å‘è€…â€ä»ç›´æ¥ä½¿ç”¨ LLM API å¼€å§‹â€ (å½“ç„¶ï¼Œå„æœ‰å„çš„ç«‹åœº)ï¼Œè€Œéæ¡†æ¶ã€‚Octomind ç­‰å…¬å¸åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ¡†æ¶12ä¸ªæœˆåé€‰æ‹©å®Œå…¨ç§»é™¤ï¼Œç§°â€ç§»é™¤åå›¢é˜Ÿæ›´å¿«ä¹ã€æ›´é«˜æ•ˆâ€ã€‚æ ¸å¿ƒåŸå› åœ¨äºï¼šLLM åº”ç”¨æœ¬è´¨ä¸Šåªéœ€è¦å­—ç¬¦ä¸²å¤„ç†ã€API è°ƒç”¨å’Œå¾ªç¯â€”â€”è¿™äº› Python åŸç”Ÿå°±èƒ½å¾ˆå¥½å®Œæˆã€‚æ¡†æ¶çš„é¢å¤–æŠ½è±¡å±‚å¸¸å¸¸æˆä¸ºè°ƒè¯•å™©æ¢¦å’Œçµæ´»æ€§æ·é”ã€‚

These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts â€‹â€‹and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.

We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.

Yes â€” itâ€™s an orchestrated, tool-enabled agent.
The system controls the workflow, while the LLM handles reasoning, routing, and tool selection.

In production, we usually avoid fully autonomous agents and prefer controlled orchestration for reliability and compliance.


We use an orchestrated, tool-enabled agent where the LLM routes requests to SQL, RAG, or web search, with strict guardrails, validation, and fallbacks for production reliability.

4ï¸âƒ£ å¿…é¡»è¡¥çš„ 5 ä¸ªç”Ÿäº§çº§èƒ½åŠ›ï¼ˆç¼ºä¸€ä¸ªéƒ½ä¸ç®— productionï¼‰
ğŸ”’ 1. Guardrails

content safety

PII detection

SQL injection prevention

ğŸ“Š 2. Observability

è®°å½•ï¼š

chosen route

top-k docs

SQL template

token usage

latency

ğŸ” 3. Retry & Fallback

tool error â†’ retry

confidence low â†’ RAG

everything fails â†’ human escalation

ğŸ’° 4. Cost Control

max tool calls

max tokens

route cacheï¼ˆsame intent reuseï¼‰

ğŸ§ª 5. Evaluation

golden questions

retrieval recall

tool accuracy

hallucination rate

5ï¸âƒ£ ä½ ç°åœ¨è¿™ä¸ª agentï¼ŒJD æ€ä¹ˆè¯´æ‰â€œé«˜çº§â€

æŠŠä½ åŸæ¥çš„è¯å‡çº§æˆï¼š

â€œI designed a production-grade, tool-routed GenAI agent where the LLM dynamically selects between SQL queries, RAG pipelines, and web search, with strict guardrails, validation layers, and observability to ensure reliability and compliance.â€

è¿™å¥è¯ éå¸¸ä¼ä¸šï¼Œéå¸¸åŠ åˆ†ã€‚
For authoritative, single-source queries such as contacts or IDs, we intentionally bypassed LLM generation and returned structured SQL results directly to ensure accuracy, brevity, and user trust.
The LLM was used strictly for intent routing rather than answer generation.


In enterprise settings, I would build RAG using Azure Cognitive Search as the vector store for retrieval, then assemble retrieved chunks into a structured prompt in Python, and call Azure OpenAI for the final answer.
I would leverage a framework like LangChain or an internal orchestration library to manage the retrieval-generation workflow, ensure guardrails, logging, and integrate it with CI/CD pipelines for production deployment.


A data lake is a cloud-scale, immutable tape archive with modern indexing and access control.

We store original invoice documents immutably in a data lake so extraction logic can evolve without losing historical accuracy



5-nano
import os
from openai import AzureOpenAI

client = AzureOpenAI(
    api_version="2024-12-01-preview",
    azure_endpoint="https://haystacked.cognitiveservices.azure.com/",
    api_key=subscription_key
)


import os
from openai import AzureOpenAI

endpoint = "https://haystacked.cognitiveservices.azure.com/"
model_name = "gpt-5-nano"
deployment = "gpt-5-nano"

subscription_key = "<your-api-key>"
api_version = "2024-12-01-preview"

client = AzureOpenAI(
    api_version=api_version,
    azure_endpoint=endpoint,
    api_key=subscription_key,
)

response = client.chat.completions.create(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant.",
        },
        {
            "role": "user",
            "content": "I am going to Paris, what should I see?",
        }
    ],
    max_completion_tokens=16384,
    model=deployment
)

print(response.choices[0].message.content)


# fastapicloud_dividend

A project created with FastAPI CLI.

## Quick Start

### Start the development server

```bash
uv run fastapi dev
```

Visit http://localhost:8000

### Deploy to FastAPI Cloud

> FastAPI Cloud is currently in private beta. Join the waitlist at https://fastapicloud.com

```bash
uv run fastapi login
uv run fastapi deploy
```

## Project Structure

- `main.py` - Your FastAPI application
- `pyproject.toml` - Project dependencies

## Learn More

- [FastAPI Documentation](https://fastapi.tiangolo.com)
- [FastAPI Cloud](https://fastapicloud.com)
åœ¨çœŸå®å·¥ç¨‹é¡¹ç›®ä¸­ï¼Œä¸ä½¿ç”¨ LangChainã€LangGraphã€CrewAI ç­‰æ¡†æ¶ï¼Œçº¯ç”¨ Python + LLM API æ‰‹åŠ¨å®ç° AI Agent ä¸ä»…å®Œå…¨å¯è¡Œï¼Œè€Œä¸”åœ¨è®¸å¤šåœºæ™¯ä¸‹æ˜¯æ›´ä¼˜é€‰æ‹©ã€‚ Anthropic å®˜æ–¹æ˜ç¡®å»ºè®®å¼€å‘è€…â€ä»ç›´æ¥ä½¿ç”¨ LLM API å¼€å§‹â€ (å½“ç„¶ï¼Œå„æœ‰å„çš„ç«‹åœº)ï¼Œè€Œéæ¡†æ¶ã€‚Octomind ç­‰å…¬å¸åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ¡†æ¶12ä¸ªæœˆåé€‰æ‹©å®Œå…¨ç§»é™¤ï¼Œç§°â€ç§»é™¤åå›¢é˜Ÿæ›´å¿«ä¹ã€æ›´é«˜æ•ˆâ€ã€‚æ ¸å¿ƒåŸå› åœ¨äºï¼šLLM åº”ç”¨æœ¬è´¨ä¸Šåªéœ€è¦å­—ç¬¦ä¸²å¤„ç†ã€API è°ƒç”¨å’Œå¾ªç¯â€”â€”è¿™äº› Python åŸç”Ÿå°±èƒ½å¾ˆå¥½å®Œæˆã€‚æ¡†æ¶çš„é¢å¤–æŠ½è±¡å±‚å¸¸å¸¸æˆä¸ºè°ƒè¯•å™©æ¢¦å’Œçµæ´»æ€§æ·é”ã€‚

These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts â€‹â€‹and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.

We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.

ä½œè€…ï¼šé™¶åˆš
é“¾æ¥ï¼šhttps://www.zhihu.com/question/1985692342427088079/answer/1994349202730394376
æ¥æºï¼šçŸ¥ä¹
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

ç†è§£æ¡†æ¶çš„ä»·å€¼éœ€è¦å…ˆçœ‹æ¸…å®ƒä»¬è¯•å›¾è§£å†³çš„æ ¸å¿ƒç—›ç‚¹ã€‚LangChain çš„æ ¸å¿ƒæŠ½è±¡åŒ…æ‹¬ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼ˆé¿å…ä¾›åº”å•†é”å®šï¼‰ã€Chainsï¼ˆå¤šæ­¥éª¤ä¸²è”ï¼‰ã€Agentsï¼ˆè‡ªä¸»å†³ç­–ï¼‰ã€Toolsï¼ˆå¤–éƒ¨ç³»ç»Ÿè¿æ¥ï¼‰å’Œ Memoryï¼ˆçŠ¶æ€ç®¡ç†ï¼‰ã€‚è¿™äº›æŠ½è±¡ç¡®å®è§£å†³äº†çœŸå®é—®é¢˜ï¼šä¸åŒ LLM API æ ¼å¼ä¸ç»Ÿä¸€ã€å•æ¬¡è°ƒç”¨æ— æ³•å®Œæˆå¤æ‚ä»»åŠ¡ã€LLM æ— æ³•æ‰§è¡Œå®é™…åŠ¨ä½œã€‚LangGraph åˆ™ä¸“æ³¨äºçŠ¶æ€å›¾å·¥ä½œæµç¼–æ’ï¼Œæä¾› StateGraphã€æ¡ä»¶è¾¹ã€æ£€æŸ¥ç‚¹å’Œ Human-in-the-loop ç­‰èƒ½åŠ›ã€‚å®ƒè§£å†³äº†å¤æ‚åˆ†æ”¯é€»è¾‘çš„è¡¨è¾¾ã€é•¿è¿è¡Œä»»åŠ¡çš„æ–­ç‚¹æ¢å¤ç­‰é«˜çº§éœ€æ±‚ã€‚CrewAI èšç„¦å¤š Agent åä½œï¼Œé€šè¿‡è§’è‰²å®šä¹‰ï¼ˆRole-Goal-Backstory æ¡†æ¶ï¼‰å’Œä»»åŠ¡å§”æ‰˜æœºåˆ¶å®ç°å›¢é˜Ÿåä½œæ¨¡å¼ã€‚AutoGPT åˆ™å°è¯•å®ç°å®Œå…¨è‡ªä¸»çš„ AI ä»£ç†ï¼Œå…·å¤‡è‡ªæˆ‘æç¤ºå’Œåæ€èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¡†æ¶ä¹Ÿå¸¦æ¥æ˜¾è‘—ä»£ä»·ã€‚LangChain è¢«æ‰¹è¯„ä¸ºâ€æŠ½è±¡å¥—æŠ½è±¡â€ï¼Œè°ƒè¯•æ—¶éœ€è¦ç†è§£å¤§é‡å†…éƒ¨æ¡†æ¶ä»£ç ï¼›LangGraph å­¦ä¹ æ›²çº¿é™¡å³­ï¼Œéœ€è¦ç†è§£å›¾è®ºæ¦‚å¿µï¼›CrewAI çš„å¤š Agent è°ƒç”¨æˆæœ¬æ˜¯ LangGraph çš„ 2-3 å€ï¼›AutoGPT åˆ™ç»å¸¸é™·å…¥æ— é™å¾ªç¯ï¼ŒOpenAI å®˜æ–¹éƒ½ç§°ç±»ä¼¼ç³»ç»Ÿåªæ˜¯â€æ•™è‚²æ€§è´¨â€ã€‚

Octomind æ¡ˆä¾‹æ˜¯æœ€å…·è¯´æœåŠ›çš„ç”Ÿäº§ç¯å¢ƒå®è·µã€‚è¿™å®¶ AI æµ‹è¯•è‡ªåŠ¨åŒ–å…¬å¸ä½¿ç”¨ LangChain è¶…è¿‡12ä¸ªæœˆåå†³å®šå®Œå…¨ç§»é™¤ã€‚è¿ç§»åŸå› åŒ…æ‹¬ï¼šæŠ½è±¡å±‚è¿‡æ·±å¯¼è‡´è°ƒè¯•å›°éš¾ã€æ— æ³•å®ç°â€åŠ¨æ€è°ƒæ•´ Agent å¯ç”¨å·¥å…·â€ç­‰éœ€æ±‚ã€API é¢‘ç¹å˜æ›´å¸¦æ¥çš„ç»´æŠ¤è´Ÿæ‹…ã€‚ç§»é™¤åçš„åé¦ˆæ˜¯ï¼š"ä¸€æ—¦æˆ‘ä»¬ç§»é™¤äº†å®ƒï¼ˆLangChainï¼‰ï¼Œå°±ä¸å†éœ€è¦æŠŠæˆ‘ä»¬çš„éœ€æ±‚ç¿»è¯‘æˆ LangChain è®¤å¯çš„æ¦‚å¿µã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥å†™ä»£ç äº†ã€‚"

å¤§å¤šæ•° LLM åº”ç”¨æ‰€éœ€è¦çš„ï¼Œæ— éå°±æ˜¯å­—ç¬¦ä¸²å¤„ç†ã€API è°ƒç”¨ã€å¾ªç¯ï¼Œå¦‚æœåš RAG çš„è¯å¯èƒ½å†åŠ ä¸ªå‘é‡æ•°æ®åº“ã€‚ä½ æ ¹æœ¬ä¸éœ€è¦å¥½å‡ å±‚æŠ½è±¡å’Œä¸€å¤§å †ä¾èµ–ï¼Œæ¥ç®¡ç†åŸºæœ¬çš„å­—ç¬¦ä¸²æ‹¼æ¥ã€HTTP è¯·æ±‚å’Œ for/while å¾ªç¯ã€‚


Simon Willisonï¼ˆDjango åˆ›å§‹äººï¼‰å¯¹ Agent çš„å®šä¹‰æä¸ºç®€æ´ï¼šâ€An LLM agent runs tools in a loop to achieve a goal.â€ ä»–è‡ªå·±å¼€å‘äº†è½»é‡çº§ LLM CLI å·¥å…·ï¼Œè€Œéä½¿ç”¨é‡å‹æ¡†æ¶ã€‚

Chip Huyenï¼ˆML å·¥ç¨‹ä¸“å®¶ï¼‰åˆ™è­¦å‘Šè¿‡æ—©å¼•å…¥æ¡†æ¶ä¼šâ€æŠ½è±¡æ‰å…³é”®ç»†èŠ‚ï¼Œä½¿è°ƒè¯•å›°éš¾â€ï¼Œå¹¶å¼ºè°ƒå¤åˆè¯¯å·®é—®é¢˜ï¼šè‹¥æ¯æ­¥å‡†ç¡®ç‡95%ï¼Œ10æ­¥åå‡†ç¡®ç‡å°†é™è‡³60%ã€‚

Max Woolf åœ¨åšå®¢ä¸­å°–é”æ‰¹è¯„ï¼š"LangChain çš„é—®é¢˜åœ¨äºï¼Œå®ƒæŠŠç®€å•çš„äº‹æƒ…å˜å¾—ç›¸å¯¹å¤æ‚â€¦â€¦è¿™ç§ Agent å·¥ä½œæµå°±åƒä¸€åº§è„†å¼±çš„çº¸ç‰Œå±‹ï¼Œå‡­è‰¯å¿ƒè®²ï¼Œæˆ‘æ²¡æ³•æŠŠå®ƒéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ã€‚"LangChain CEO Harrison Chase æ‰¿è®¤æ—©æœŸç‰ˆæœ¬â€æŠ½è±¡è¿‡åº¦â€ï¼Œå¹¶æ¨å‡º LangGraph ä½œä¸ºæ›´åº•å±‚çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä»–å°†æ¡†æ¶ç±»æ¯”ä¸º Kerasâ€”â€”æä¾›é«˜å±‚å°è£…ä¾¿äºä¸Šæ‰‹ï¼Œä½†å…³é”®æ˜¯è¦æ„å»ºåœ¨æ›´ä½å±‚æŠ½è±¡ä¹‹ä¸Š


