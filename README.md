# fastapicloud_dividend

A project created with FastAPI CLI.

## Quick Start

### Start the development server

```bash
uv run fastapi dev
```

Visit http://localhost:8000

### Deploy to FastAPI Cloud

> FastAPI Cloud is currently in private beta. Join the waitlist at https://fastapicloud.com

```bash
uv sync
uv run fastapi login
uv run fastapi deploy
```

## Project Structure

- `main.py` - Your FastAPI application
- `pyproject.toml` - Project dependencies

## Learn More

- [FastAPI Documentation](https://fastapi.tiangolo.com)
- [FastAPI Cloud](https://fastapicloud.com)
åœ¨çœŸå®å·¥ç¨‹é¡¹ç›®ä¸­ï¼Œä¸ä½¿ç”¨ LangChainã€LangGraphã€CrewAI ç­‰æ¡†æ¶ï¼Œçº¯ç”¨ Python + LLM API æ‰‹åŠ¨å®ç° AI Agent ä¸ä»…å®Œå…¨å¯è¡Œï¼Œè€Œä¸”åœ¨è®¸å¤šåœºæ™¯ä¸‹æ˜¯æ›´ä¼˜é€‰æ‹©ã€‚ Anthropic å®˜æ–¹æ˜ç¡®å»ºè®®å¼€å‘è€…â€ä»ç›´æ¥ä½¿ç”¨ LLM API å¼€å§‹â€ (å½“ç„¶ï¼Œå„æœ‰å„çš„ç«‹åœº)ï¼Œè€Œéæ¡†æ¶ã€‚Octomind ç­‰å…¬å¸åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ¡†æ¶12ä¸ªæœˆåé€‰æ‹©å®Œå…¨ç§»é™¤ï¼Œç§°â€ç§»é™¤åå›¢é˜Ÿæ›´å¿«ä¹ã€æ›´é«˜æ•ˆâ€ã€‚æ ¸å¿ƒåŸå› åœ¨äºï¼šLLM åº”ç”¨æœ¬è´¨ä¸Šåªéœ€è¦å­—ç¬¦ä¸²å¤„ç†ã€API è°ƒç”¨å’Œå¾ªç¯â€”â€”è¿™äº› Python åŸç”Ÿå°±èƒ½å¾ˆå¥½å®Œæˆã€‚æ¡†æ¶çš„é¢å¤–æŠ½è±¡å±‚å¸¸å¸¸æˆä¸ºè°ƒè¯•å™©æ¢¦å’Œçµæ´»æ€§æ·é”ã€‚

These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts â€‹â€‹and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.

We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.

Yes â€” itâ€™s an orchestrated, tool-enabled agent.
The system controls the workflow, while the LLM handles reasoning, routing, and tool selection.

In production, we usually avoid fully autonomous agents and prefer controlled orchestration for reliability and compliance.


We use an orchestrated, tool-enabled agent where the LLM routes requests to SQL, RAG, or web search, with strict guardrails, validation, and fallbacks for production reliability.

4ï¸âƒ£ å¿…é¡»è¡¥çš„ 5 ä¸ªç”Ÿäº§çº§èƒ½åŠ›ï¼ˆç¼ºä¸€ä¸ªéƒ½ä¸ç®— productionï¼‰
ğŸ”’ 1. Guardrails

content safety

PII detection

SQL injection prevention

ğŸ“Š 2. Observability

è®°å½•ï¼š

chosen route

top-k docs

SQL template

token usage

latency

ğŸ” 3. Retry & Fallback

tool error â†’ retry

confidence low â†’ RAG

everything fails â†’ human escalation

ğŸ’° 4. Cost Control

max tool calls

max tokens

route cacheï¼ˆsame intent reuseï¼‰

ğŸ§ª 5. Evaluation

golden questions

retrieval recall

tool accuracy

hallucination rate

5ï¸âƒ£ ä½ ç°åœ¨è¿™ä¸ª agentï¼ŒJD æ€ä¹ˆè¯´æ‰â€œé«˜çº§â€

æŠŠä½ åŸæ¥çš„è¯å‡çº§æˆï¼š

â€œI designed a production-grade, tool-routed GenAI agent where the LLM dynamically selects between SQL queries, RAG pipelines, and web search, with strict guardrails, validation layers, and observability to ensure reliability and compliance.â€

è¿™å¥è¯ éå¸¸ä¼ä¸šï¼Œéå¸¸åŠ åˆ†ã€‚
For authoritative, single-source queries such as contacts or IDs, we intentionally bypassed LLM generation and returned structured SQL results directly to ensure accuracy, brevity, and user trust.
The LLM was used strictly for intent routing rather than answer generation.


In enterprise settings, I would build RAG using Azure Cognitive Search as the vector store for retrieval, then assemble retrieved chunks into a structured prompt in Python, and call Azure OpenAI for the final answer.
I would leverage a framework like LangChain or an internal orchestration library to manage the retrieval-generation workflow, ensure guardrails, logging, and integrate it with CI/CD pipelines for production deployment.


A data lake is a cloud-scale, immutable tape archive with modern indexing and access control.

We store original invoice documents immutably in a data lake so extraction logic can evolve without losing historical accuracy